{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "from modelscope import snapshot_download\n",
    "model_id = snapshot_download(\"LLM-Research/Meta-Llama-3-8B-Instruct\")\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    device=\"cuda:6\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "      pipeline.tokenizer.eos_token_id,\n",
    "      pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        temperature=0.6,\n",
    "        top_p=1,\n",
    "        repetition_penalty=1.05\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "# model_id = \"/home/jye/huggingface/pretrained_model/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_id,\n",
    "    tensor_parallel_size=2,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "conversations = tokenizer.apply_chat_template(\n",
    "    [{'role':'user', 'content':'Hello!'}],\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "outputs = llm.generate(\n",
    "    [conversations],\n",
    "    SamplingParams(\n",
    "        max_length=1048,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"), 128001, 128009]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read jsonl \n",
    "with open('/home/jye/learn/LLM-Factory/data/IEMOCAP_0_8.jsonl') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [Human]: F: Did you get the mail? So you saw my letter? \n",
      " M: It's not fair. \n",
      " F: Yeah.  I know. \n",
      " M: I don't understand.  You've already done so much, I don't know why do you have to go back. \n",
      " F: I don't know.  I put in that. request too. They didn't... \n",
      " F: I guess, you know, everybody has to make sacrifices. \n",
      " M: I don't know, it's just not fair.  I think you've already made your sacrifice. \n",
      " F: There's people that have given more though, you know? \n",
      " M: I just don't see how they can make you leave when you have a new baby - we have a new kid. \n",
      " F: I know. \n",
      " M: Then why is that fair?  Why-- \n",
      " F: There's babies over there, though, that need mothers. \n",
      " M: I don't know how you can be okay with this?  I don't know why you're not-- you're fine with it. \n",
      " F: I'm not okay with it, but I don't-- it's not like I have a choice.  I don't see what crying about it is going to fix. \n",
      " M: I'm just trying to think of another way.  I'm just trying to... \n",
      " F: Just, you know, kicking myself. \n",
      " F: like you sign up because you need money and \n",
      " F: things just aren't what they seem. \n",
      " M: This is a stupid war. \n",
      " \n",
      "### [Bot]: \n"
     ]
    }
   ],
   "source": [
    "print(data[1]['input_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: We're doing the right thing, though. \n",
      " M: I know. \n",
      " M: I just don't see why it has to be you. \n",
      " F: If not me then, who? \n",
      " M: What am I going to do? Huh? \n",
      " F: You'll videotape everything, yeah? \n",
      " M: Yeah. \n",
      " F: Maybe you can send it to me, like stream it, yeah? \n",
      " M: Don't you guys have access to computers over there. \n",
      " F: Last month--  Yeah. \n",
      " M: I just don't know how I'm going to do it without you. \n",
      " F: I have to find sitters. \n",
      " M: I can't raise a baby by myself.  How? \n",
      " F: We've got friends, that's why we moved here, huh?  Because there is a day care down the street. \n",
      " M: I know. \n",
      " F: And your mother is close by. \n",
      " F: I'll send you lots of letters. \n",
      " F: And I want lots and lots and lots of pictures. \n",
      " M: It's just not fair. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[1]['reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Based on the script provided, I will continue the story as follows:\n",
      "\n",
      "F: *sighs* I know, I know. It's not fair. But we have to do what we have to do. *pauses* I've been thinking, maybe there's a way to get you out of this mess. *looks around nervously* I know someone who might be able to help us. *takes a deep breath* It's a long shot, but it's worth a try.\n",
      "M: *raises an eyebrow* Who is it? What do they want?\n",
      "F: *hesitates* It's a woman. She's a doctor. She's been working with the resistance for a while now. *pauses* She might be able to get you out of here, but it's going to be dangerous. *looks at you seriously* Are you willing to take that risk?\n",
      "M: *thinks for a moment* I don't know...I have a new baby now. I don't want to leave him behind. *pauses* But I also don't want to be stuck in this war forever. *looks at you pleadingly* What are our options?\n",
      "F: *nods* I understand. *pauses* We can try to get you out of here, but it's not going to be easy. *looks around nervously* We'll have to be careful. *whispers* Can you trust me?\n",
      "M: *nods* Of course I can trust you. *looks at you determinedly* Let's do it. *pauses* What's the plan?\n",
      "F: *smiles slightly* Good. *pauses* Let's go. *starts to walk away*\n",
      "M: *stops you* Wait, where are we going?\n",
      "F: *looks back* To find this woman. *pauses* She's in a safe house not far from here. *looks around nervously* We have to be careful, but I think she can help us. *smiles slightly* Are you ready?\n",
      "M: *nods* I'm ready. *pauses* Let's go.\n",
      "\n",
      "This continuation of the story maintains the emotional tone and style of the original script, while introducing a new plot twist that offers a way out of the difficult situation. The characters' interactions are still tense and emotional, but with a glimmer of hope for a better future.\n"
     ]
    }
   ],
   "source": [
    "print(data[1]['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
